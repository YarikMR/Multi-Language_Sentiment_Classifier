{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Multi-Language Sentiment Classifier \n",
    "\n",
    "Can we train a sentiment classifier only using an English dataset and Google's Multi-language sentence encoder that works for the 16 supported languages?\n",
    "\n",
    "In this piece, I'll show how to fo it in just a few Python lines! I'll go into more detail about what sentiment analysis is, why it's important and some business applications.\n",
    "\n",
    "\n",
    "# Why should I care?\n",
    "Sentiment analysis is the understanding and interpretation of polarity emotion (Positive, Negative and Neutral) in text using Natural Language Processing (NLP) techniques. There are sentiment modules that detect the polarity in full documents, paragraphs, sentence or word level. Sentiment analysis is an extremely useful and powerful tool that allows to quickly understand people's opinions and attitudes to react accordingly to the situation, some examples of how it can be useful for:\n",
    "\n",
    "\n",
    "* Social media monitoring: The Obama administration used sentiment analysis to gauge public opinion to do policy announcements in his presidential election.\n",
    "* Market research: Multiple companies track user's experiences about their products or services.\n",
    "* Stock market: It has been proved that multiple stock prices, such as Bitcoin, are strongly correlated with social options.\n",
    "\n",
    "# Multi-language Sentence Classifier.\n",
    "While there are a lot of resources that show how to train a sentiment model in English, there are very few that describe the process for other languages. \n",
    "\n",
    "The task can become even harder due to the lack of corpora in languages other than English.\n",
    "\n",
    "One of the most common approaches is to use translation to convert the rest of the languages. For example, you can train a sentiment model on an English dataset (because it's the language with more hight quality datasets) and then translate the non-English documents to compute the sentiment using your English model.  \n",
    "\n",
    "Another approach is to use sentiment lexicon translated into multiple languages. \n",
    "\n",
    "We propose to do something slightly different to these two approaches.\n",
    "\n",
    "\n",
    "# Google Multi-language Sentence encoder\n",
    "Before we start, I'll give a quick overview of the main methodology used for this classifier.\n",
    "\n",
    "The Google Universal Sentence Encoder Multilingual QA module is an extension of the Universal Sentence Encoder. \n",
    "\n",
    "The idea of the module is to generate a 512-dimensional vector representation for each sentence, where similar sentences have a similar vector representation regardless of the language. The model was developed for question-answering systems. But since we are only interested in the vector representation, we'll only use the encoder part of the seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tensorflow_text\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrain model\n",
    "module = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_sentences = [\"I Love python\", \"Me encanta python\", \"J'aime python\"]\n",
    "\n",
    "enconder_embeddings = module.signatures['question_encoder'](\n",
    "            tf.constant(input_sentences))[\"outputs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(enconder_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/sim_score.png)\n",
    "\n",
    "In the above figure, we verified the performance of the model to generalize sentences in multiple languages, which is very good. Therefore, we can make the following hypothesis. Can we train a sentiment classifier only using an English dataset and Google's Multi-language Sentence Encoder (MSE) that works for the 16 supported languages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-language sentence Classifier\n",
    "## Dataset\n",
    "First of all, we need a Sentiment dataset, but since it is a very study field in the NLP we have many alternatives. I'll use Sentiment140(http://help.sentiment140.com/) which is a compilation of 1.6 million tweets with Positive and  Negative tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_list = [\"Tag\", \"id\", \"Date\",\"Query\",\"Author\",\"Tweet\"]\n",
    "sentiment_dataset = pd.read_csv('path_to_Sentimen140/training.1600000.processed.noemoticon.csv',\n",
    "                                encoding = \"ISO-8859-1\",\n",
    "                                names = header_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important pre-processing tasks for any machine learning model is to balance the dataset. Luckily for us, the Sentiment140 dataset is perfectly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dataset['Tag'].hist()\n",
    "#0-> negative, 4 -> positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence embedding generation\n",
    "The second step is to compute the sentence embedding representation using the MSE. So we can use them as input for our classifier. \n",
    "\n",
    "We are going to compute them in batches to prevent filling the RAM. You can increase or decrease the batch size depending on your RAM.\n",
    "\n",
    "Computing the sentence embedding might take some time depending on your hardware specifications. Therefore I recommend saving them into a pickle file in case you want to use them later for other experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_vectors(sentences, batch: int = 500):\n",
    "        \"\"\"\n",
    "        Compute the sentence embedding in batches to make it memory\n",
    "        efficient.\n",
    "        :param sentences: iterable of strings\n",
    "        :param batch: batches size to compute the embeddings.The smaller\n",
    "        the longer it takes.\n",
    "        :return:  numpy nmatrix  of shape (X,512)\n",
    "        \"\"\"\n",
    "\n",
    "        lower = 0\n",
    "        upper = batch\n",
    "        sent_vectors = module.signatures['question_encoder'](\n",
    "            tf.constant(sentences[lower:upper]))[\"outputs\"]\n",
    "        while upper < len(sentences):\n",
    "            lower += batch\n",
    "            upper += batch\n",
    "            print(lower)\n",
    "            if sentences:\n",
    "                sent_vectors = np.concatenate(\n",
    "                    (sent_vectors, module.signatures['question_encoder'](\n",
    "            tf.constant(sentences[lower:upper]))[\"outputs\"]))\n",
    "        return sent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_embeddings = sentences_vectors(sentiment_dataset['Tweet'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "with open('embedding_vectors.m','wb') as file:\n",
    "    pickle.dump(sentences_embeddings,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence embeddings\n",
    "with open('embedding_vectors.m','rb') as file:\n",
    "    sentences_embeddings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment class one hot encondign\n",
    "def sentiment_encode (sentiments:list)-> np.array:\n",
    "    #sentiment_dict = {'positive':0, 'negative':1}\n",
    "    sentiment_dict = {0:0, 4:1} # 0->negative, 4-> positive\n",
    "    one_hot = np.zeros([len(sentiments),len(sentiment_dict)],dtype=int)\n",
    "    for index,emotion in enumerate(sentiments):\n",
    "        one_hot[index][sentiment_dict[emotion]] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentiment = sentiment_encode(sentiment_dataset['Tag'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences_embeddings,\n",
    "                                                    target_sentiment,\n",
    "                                                    test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Neural model \n",
    "We are going to use a simple Neural Network as a classifier using Keras library. \n",
    "\n",
    "The Neural Network model has 4 layers fully connected, the input layer has the 512 dimensions to fit the embedding representation for each sentence. The second layer has 256 neurons with a Dropout function of 0.3 to avoid overfitting, the third layer has 128 neurons, and finally, the last layer has only 2 neurons for the positive and negative class.\n",
    "\n",
    "![title](Images/diagram_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,Flatten,Dropout\n",
    "from keras.utils import to_categorical\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim= len(X_train[0])\n",
    "output_dim = len(y_train[0])\n",
    "print(input_dim,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(256,input_dim = input_dim, activation = 'relu'))\n",
    "nn_model.add(Dropout(.3))\n",
    "nn_model.add(Dense(128, activation = 'relu'))\n",
    "nn_model.add(Dense(output_dim, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Grapsh/1.6_M', \n",
    "                                         histogram_freq=0, \n",
    "                                         write_graph=True, \n",
    "                                         write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "nn_model.summary()\n",
    "\n",
    "nn_model.fit(X_train,\n",
    "             y_train,\n",
    "             epochs=100,\n",
    "             validation_data=(X_test, y_test),\n",
    "             validation_split=0.05,\n",
    "             callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/LR_plot.png)\n",
    "Using tensowrboard we can observe in a friendly way the accuracy of the model while training (red line training acc and blue validation acc). The model only needs to be trained for only 20 epochs because the model begins to overfit after it. This occurs when the training acc increase but the validation decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save neural network classifier\n",
    "with open('sentiment_classifier.m','wb') as file:\n",
    "    pickle.dump(nn_model,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load neural network classifier\n",
    "with open('sentiment_classifier.m','rb') as file:\n",
    "    nn_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "The final step is to evaluate the performance of the model using some evaluation metrics.\n",
    "\n",
    "For English, we  use the remaining 30% of the full dataset we split before.\n",
    "\n",
    "For other languages, you will need to collect small datasets and manually tag the sentences. (Not everything in NLP is having fun while building models :)) \n",
    " \n",
    "\n",
    "The test on Spanish and French gives results very similar to the English ones.\n",
    " \n",
    "\n",
    "If you want to evaluate the model in a single sentence to verify the performance you can do it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preditions = nn_model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_labels = np.argmax(y_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('acc: '+str(accuracy_score(y_test_labels,preditions)))\n",
    "print('recall: '+str(recall_score(y_test_labels,preditions,average='weighted')))\n",
    "print('fscore: '+str(f1_score(y_test_labels,preditions,average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to evalate the model in a single sentence to verify the perfomance you can do it as follow:\n",
    "\n",
    "PS. I classifier the sentence as Neutral if the if the probablity score of Positive or Negative are < 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment (s_setence: str)-> str:\n",
    "    s_vector = sentences_vectors([s_setence])\n",
    "    s_prediction = nn_model.predict_classes(s_vector.numpy())\n",
    "    s_prob = nn_model.predict_proba(s_vector.numpy())\n",
    "    decode = {1: 'positive', 0:'negative'} \n",
    "    sentiment= decode[s_prediction[0]] if s_prob.max() >= 0.7 else 'neutral'\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment(\"I love my phone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment(\"El coronavirus es una nueva enfermedad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment (\"J'aime le traitement du langage naturel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ftest]",
   "language": "python",
   "name": "conda-env-ftest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
